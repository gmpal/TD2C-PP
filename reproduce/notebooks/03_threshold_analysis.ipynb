{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avoids the need for users to install TD2C as a package\n",
    "import sys\n",
    "sys.path.append('../../') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "--- Finding Robust Threshold via Leave-One-Process-Out CV ---\n",
      "--- Calculating for FOUR different optimization metrics ---\n",
      "Verified 9 unique process groups.\n",
      "\n",
      "Starting Leave-One-Group-Out CV across 9 processes...\n",
      "----------------------------------------\n",
      "Fold 1: Holding out process number '1'\n",
      "  - Training classifier...\n",
      "  - Predicting probabilities and finding optimal thresholds...\n",
      "  - Thresholds found: F1=0.280, BEP=0.300, J=0.200, Dist=0.200\n",
      "----------------------------------------\n",
      "Fold 2: Holding out process number '3'\n",
      "  - Training classifier...\n",
      "  - Predicting probabilities and finding optimal thresholds...\n",
      "  - Thresholds found: F1=0.140, BEP=0.880, J=0.140, Dist=0.120\n",
      "----------------------------------------\n",
      "Fold 3: Holding out process number '5'\n",
      "  - Training classifier...\n",
      "  - Predicting probabilities and finding optimal thresholds...\n",
      "  - Thresholds found: F1=0.400, BEP=0.840, J=0.340, Dist=0.260\n",
      "----------------------------------------\n",
      "Fold 4: Holding out process number '7'\n",
      "  - Training classifier...\n",
      "  - Predicting probabilities and finding optimal thresholds...\n",
      "  - Thresholds found: F1=0.520, BEP=0.660, J=0.360, Dist=0.400\n",
      "----------------------------------------\n",
      "Fold 5: Holding out process number '9'\n",
      "  - Training classifier...\n",
      "  - Predicting probabilities and finding optimal thresholds...\n",
      "  - Thresholds found: F1=0.940, BEP=0.500, J=0.920, Dist=0.220\n",
      "----------------------------------------\n",
      "Fold 6: Holding out process number '11'\n",
      "  - Training classifier...\n",
      "  - Predicting probabilities and finding optimal thresholds...\n",
      "  - Thresholds found: F1=0.660, BEP=0.920, J=0.440, Dist=0.520\n",
      "----------------------------------------\n",
      "Fold 7: Holding out process number '13'\n",
      "  - Training classifier...\n",
      "  - Predicting probabilities and finding optimal thresholds...\n",
      "  - Thresholds found: F1=0.200, BEP=0.060, J=0.040, Dist=0.020\n",
      "----------------------------------------\n",
      "Fold 8: Holding out process number '15'\n",
      "  - Training classifier...\n",
      "  - Predicting probabilities and finding optimal thresholds...\n",
      "  - Thresholds found: F1=0.340, BEP=0.580, J=0.260, Dist=0.300\n",
      "----------------------------------------\n",
      "Fold 9: Holding out process number '19'\n",
      "  - Training classifier...\n",
      "  - Predicting probabilities and finding optimal thresholds...\n",
      "  - Thresholds found: F1=0.820, BEP=0.820, J=0.740, Dist=0.740\n",
      "\n",
      "============================================================\n",
      "--- Cross-Validation Complete: Comparison of Metrics ---\n",
      "          Metric  Avg. Threshold  Std. Dev.\n",
      "    Max F1-Score        0.477778   0.262711\n",
      "Break-Even (P≈R)        0.617778   0.274784\n",
      "      Youden's J        0.382222   0.268070\n",
      "Closest to (0,1)        0.308889   0.205504\n",
      "============================================================\n",
      "\n",
      "--- Interpretation ---\n",
      "This table shows the average 'optimal' threshold according to four different definitions of optimality.\n",
      "A low standard deviation for a given metric suggests its result is stable across different processes.\n",
      "The average thresholds differ, indicating your choice of 'optimality' significantly impacts the result.\n",
      "\n",
      "--- Recommendation ---\n",
      "1. For your final paper, choose ONE metric and justify it. Maximizing the F1-Score is the most standard and defensible choice.\n",
      "2. You can use this table in an appendix or 'sensitivity analysis' section to show that you explored other options.\n",
      "3. If another metric (e.g., Youden's J) gives you a threshold that you believe is more practical for your problem, you can use it, but you must clearly state why you chose it over the F1-score.\n"
     ]
    }
   ],
   "source": [
    "# find_robust_threshold_ALL_METRICS.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "DESCRIPTORS_DIR = Path('data/descriptors/')\n",
    "TRAIN_DESCRIPTORS_FILE = 'descriptors_df_train.pkl'\n",
    "N_JOBS = 50\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. DATA LOADING AND PREPARATION ---\n",
    "# ==============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"--- Finding Robust Threshold via Leave-One-Process-Out CV ---\")\n",
    "print(\"--- Calculating for FOUR different optimization metrics ---\")\n",
    "\n",
    "# ... (same data loading and group assignment code as before) ...\n",
    "training_data_path = DESCRIPTORS_DIR / TRAIN_DESCRIPTORS_FILE\n",
    "descriptors_df_train = pd.read_pickle(training_data_path)\n",
    "descriptors_df_train.fillna(0, inplace=True)\n",
    "train_process_numbers = [1, 3, 5, 7, 9, 11, 13, 15, 19]\n",
    "def get_process_num(graph_id):\n",
    "    process_idx = (graph_id % 1080) // 120\n",
    "    return train_process_numbers[process_idx]\n",
    "descriptors_df_train['process_num'] = descriptors_df_train['graph_id'].apply(get_process_num)\n",
    "print(f\"Verified {len(descriptors_df_train['process_num'].unique())} unique process groups.\")\n",
    "X = descriptors_df_train.drop(columns=['graph_id', 'edge_source', 'edge_dest', 'is_causal', 'process_num'])\n",
    "y = descriptors_df_train['is_causal']\n",
    "groups = descriptors_df_train['process_num']\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. LEAVE-ONE-GROUP-OUT (PROCESS) CROSS-VALIDATION ---\n",
    "# ==============================================================================\n",
    "logo = LeaveOneGroupOut()\n",
    "# Initialize lists to store thresholds for each metric\n",
    "thresholds_f1, thresholds_be, thresholds_j, thresholds_dist = [], [], [], []\n",
    "\n",
    "print(f\"\\nStarting Leave-One-Group-Out CV across {logo.get_n_splits(groups=groups)} processes...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(logo.split(X, y, groups)):\n",
    "    held_out_process = groups.iloc[val_idx].unique()[0]\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Fold {fold+1}: Holding out process number '{held_out_process}'\")\n",
    "\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    print(f\"  - Training classifier...\")\n",
    "    clf_fold = BalancedRandomForestClassifier(\n",
    "        n_estimators=50, max_depth=None, random_state=42,\n",
    "        sampling_strategy='auto', replacement=True, bootstrap=True, n_jobs=N_JOBS\n",
    "    )\n",
    "    clf_fold.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    print(f\"  - Predicting probabilities and finding optimal thresholds...\")\n",
    "    y_proba_val_fold = clf_fold.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "    # --- METRIC 1: Max F1-Score ---\n",
    "    precision, recall, pr_thresh = precision_recall_curve(y_val_fold, y_proba_val_fold)\n",
    "    f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-9)\n",
    "    thresholds_f1.append(pr_thresh[np.argmax(f1_scores)])\n",
    "    \n",
    "    # --- METRIC 2: Break-Even Point (P≈R) ---\n",
    "    diffs = np.abs(precision[:-1] - recall[:-1])\n",
    "    thresholds_be.append(pr_thresh[np.argmin(diffs)])\n",
    "\n",
    "    # --- METRICS 3 & 4 (from ROC curve) ---\n",
    "    fpr, tpr, roc_thresh = roc_curve(y_val_fold, y_proba_val_fold)\n",
    "    \n",
    "    # --- METRIC 3: Youden's J Statistic ---\n",
    "    thresholds_j.append(roc_thresh[np.argmax(tpr - fpr)])\n",
    "\n",
    "    # --- METRIC 4: Closest to (0,1) ---\n",
    "    distances = np.sqrt(fpr**2 + (1-tpr)**2)\n",
    "    thresholds_dist.append(roc_thresh[np.argmin(distances)])\n",
    "\n",
    "    print(f\"  - Thresholds found: F1={thresholds_f1[-1]:.3f}, BEP={thresholds_be[-1]:.3f}, J={thresholds_j[-1]:.3f}, Dist={thresholds_dist[-1]:.3f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. FINAL RESULTS AND COMPARISON ---\n",
    "# ==============================================================================\n",
    "results = {\n",
    "    \"Metric\": [\"Max F1-Score\", \"Break-Even (P≈R)\", \"Youden's J\", \"Closest to (0,1)\"],\n",
    "    \"Avg. Threshold\": [np.mean(thresholds_f1), np.mean(thresholds_be), np.mean(thresholds_j), np.mean(thresholds_dist)],\n",
    "    \"Std. Dev.\": [np.std(thresholds_f1), np.std(thresholds_be), np.std(thresholds_j), np.std(thresholds_dist)]\n",
    "}\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"--- Cross-Validation Complete: Comparison of Metrics ---\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- Interpretation and Recommendation ---\n",
    "print(\"\\n--- Interpretation ---\")\n",
    "print(\"This table shows the average 'optimal' threshold according to four different definitions of optimality.\")\n",
    "print(\"A low standard deviation for a given metric suggests its result is stable across different processes.\")\n",
    "if results_df['Avg. Threshold'].std() < 0.05:\n",
    "    print(\"The average thresholds are all very similar, indicating the choice of metric is not critical.\")\n",
    "else:\n",
    "    print(\"The average thresholds differ, indicating your choice of 'optimality' significantly impacts the result.\")\n",
    "    \n",
    "print(\"\\n--- Recommendation ---\")\n",
    "print(\"1. For your final paper, choose ONE metric and justify it. Maximizing the F1-Score is the most standard and defensible choice.\")\n",
    "print(\"2. You can use this table in an appendix or 'sensitivity analysis' section to show that you explored other options.\")\n",
    "print(\"3. If another metric (e.g., Youden's J) gives you a threshold that you believe is more practical for your problem, you can use it, but you must clearly state why you chose it over the F1-score.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "td2c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
