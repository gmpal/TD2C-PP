{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avoids the need for users to install TD2C as a package\n",
    "import sys\n",
    "sys.path.append('../..') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TEST dataset from data/causal_dfs/causal_dfs_TEST.pkl\n",
      "Found 1080 runs in the dataset\n",
      "\n",
      "=== ANALYZING BY ERROR TYPE ===\n",
      "\n",
      "Processing error type: gaussian\n",
      "Micro-averaged results for gaussian:\n",
      "            Accuracy  Balanced Accuracy  Precision  Recall  F1-Score\n",
      "Method                                                              \n",
      "VAR           0.8391             0.5411     0.4375  0.1080    0.1732\n",
      "VARLiNGAM     0.7904             0.6964     0.3827  0.5598    0.4546\n",
      "PCMCI         0.8684             0.7356     0.5847  0.5425    0.5628\n",
      "MVGC          0.8624             0.7416     0.5585  0.5660    0.5622\n",
      "PCMCI-GPDC    0.8707             0.7152     0.6065  0.4891    0.5415\n",
      "Granger       0.7149             0.4579     0.0847  0.0842    0.0845\n",
      "DYNOTEARS     0.8011             0.5183     0.2194  0.1073    0.1441\n",
      "D2C           0.8539             0.8048     0.5227  0.7335    0.6104\n",
      "\n",
      "Macro-averaged results for gaussian:\n",
      "Metric             Accuracy Balanced Accuracy         F1-Score        Precision           Recall\n",
      "Method                                                                                          \n",
      "D2C         0.8539 ± 0.0916   0.8185 ± 0.1355  0.6238 ± 0.2068  0.5531 ± 0.2230  0.7625 ± 0.2314\n",
      "DYNOTEARS   0.8011 ± 0.1441   0.5273 ± 0.0703  0.0884 ± 0.1846  0.1387 ± 0.3256  0.1211 ± 0.2369\n",
      "Granger     0.7149 ± 0.1297   0.4502 ± 0.0610  0.0635 ± 0.0830  0.0731 ± 0.1112  0.0721 ± 0.1009\n",
      "MVGC        0.8624 ± 0.0742   0.7392 ± 0.1969  0.4830 ± 0.3275  0.4796 ± 0.2947  0.5623 ± 0.4137\n",
      "PCMCI       0.8684 ± 0.0728   0.7364 ± 0.2012  0.4978 ± 0.3350  0.4986 ± 0.2975  0.5447 ± 0.4022\n",
      "PCMCI-GPDC  0.8707 ± 0.0688   0.7209 ± 0.1943  0.4778 ± 0.3310  0.5084 ± 0.2989  0.5009 ± 0.3899\n",
      "VAR         0.8391 ± 0.0586   0.5411 ± 0.0598  0.1551 ± 0.1638  0.3516 ± 0.3656  0.1083 ± 0.1236\n",
      "VARLiNGAM   0.7904 ± 0.1335   0.7079 ± 0.1917  0.4412 ± 0.2890  0.4036 ± 0.2560  0.5820 ± 0.3739\n",
      "\n",
      "Processing error type: uniform\n",
      "Micro-averaged results for uniform:\n",
      "            Accuracy  Balanced Accuracy  Precision  Recall  F1-Score\n",
      "Method                                                              \n",
      "VAR           0.8359             0.5347     0.3910  0.0973    0.1558\n",
      "VARLiNGAM     0.7938             0.6954     0.3865  0.5525    0.4548\n",
      "PCMCI         0.8641             0.7242     0.5693  0.5211    0.5441\n",
      "MVGC          0.8631             0.7361     0.5615  0.5515    0.5565\n",
      "PCMCI-GPDC    0.8662             0.7038     0.5882  0.4680    0.5213\n",
      "Granger       0.7143             0.4550     0.0791  0.0785    0.0788\n",
      "DYNOTEARS     0.7996             0.5162     0.2108  0.1047    0.1399\n",
      "D2C           0.8367             0.7973     0.4839  0.7402    0.5852\n",
      "\n",
      "Macro-averaged results for uniform:\n",
      "Metric             Accuracy Balanced Accuracy         F1-Score        Precision           Recall\n",
      "Method                                                                                          \n",
      "D2C         0.8367 ± 0.1102   0.8136 ± 0.1440  0.6126 ± 0.2252  0.5396 ± 0.2521  0.7745 ± 0.2322\n",
      "DYNOTEARS   0.7996 ± 0.1471   0.5262 ± 0.0699  0.0847 ± 0.1823  0.1263 ± 0.3129  0.1202 ± 0.2387\n",
      "Granger     0.7143 ± 0.1275   0.4463 ± 0.0575  0.0568 ± 0.0821  0.0599 ± 0.0906  0.0648 ± 0.0995\n",
      "MVGC        0.8631 ± 0.0769   0.7348 ± 0.2022  0.4709 ± 0.3385  0.4666 ± 0.3109  0.5498 ± 0.4224\n",
      "PCMCI       0.8641 ± 0.0738   0.7258 ± 0.2068  0.4729 ± 0.3410  0.4738 ± 0.3000  0.5250 ± 0.4150\n",
      "PCMCI-GPDC  0.8662 ± 0.0704   0.7107 ± 0.1976  0.4571 ± 0.3367  0.4857 ± 0.3122  0.4820 ± 0.3942\n",
      "VAR         0.8359 ± 0.0624   0.5328 ± 0.0556  0.1339 ± 0.1532  0.3054 ± 0.3489  0.0941 ± 0.1168\n",
      "VARLiNGAM   0.7938 ± 0.1384   0.7076 ± 0.1988  0.4393 ± 0.3073  0.4014 ± 0.2714  0.5758 ± 0.3872\n",
      "\n",
      "Processing error type: laplace\n",
      "Micro-averaged results for laplace:\n",
      "            Accuracy  Balanced Accuracy  Precision  Recall  F1-Score\n",
      "Method                                                              \n",
      "VAR           0.8363             0.5367     0.3983  0.1016    0.1620\n",
      "VARLiNGAM     0.7943             0.7102     0.3926  0.5880    0.4708\n",
      "PCMCI         0.8697             0.7417     0.5855  0.5558    0.5703\n",
      "MVGC          0.8677             0.7535     0.5729  0.5877    0.5802\n",
      "PCMCI-GPDC    0.8743             0.7361     0.6093  0.5356    0.5701\n",
      "Granger       0.7172             0.4581     0.0835  0.0819    0.0827\n",
      "DYNOTEARS     0.8001             0.5301     0.2463  0.1381    0.1769\n",
      "D2C           0.8693             0.8181     0.5601  0.7439    0.6391\n",
      "\n",
      "Macro-averaged results for laplace:\n",
      "Metric             Accuracy Balanced Accuracy         F1-Score        Precision           Recall\n",
      "Method                                                                                          \n",
      "D2C         0.8693 ± 0.0855   0.8333 ± 0.1225  0.6554 ± 0.2034  0.5983 ± 0.2311  0.7753 ± 0.2083\n",
      "DYNOTEARS   0.8001 ± 0.1583   0.5441 ± 0.0800  0.1399 ± 0.2110  0.2766 ± 0.4279  0.1613 ± 0.2524\n",
      "Granger     0.7172 ± 0.1263   0.4497 ± 0.0573  0.0619 ± 0.0837  0.0691 ± 0.1098  0.0689 ± 0.0983\n",
      "MVGC        0.8677 ± 0.0686   0.7488 ± 0.1828  0.5120 ± 0.3055  0.5088 ± 0.2930  0.5792 ± 0.3799\n",
      "PCMCI       0.8697 ± 0.0700   0.7407 ± 0.1915  0.5170 ± 0.3132  0.5204 ± 0.2980  0.5546 ± 0.3768\n",
      "PCMCI-GPDC  0.8743 ± 0.0617   0.7379 ± 0.1852  0.5175 ± 0.3064  0.5453 ± 0.2953  0.5395 ± 0.3676\n",
      "VAR         0.8363 ± 0.0613   0.5356 ± 0.0513  0.1442 ± 0.1403  0.3484 ± 0.3408  0.1001 ± 0.1075\n",
      "VARLiNGAM   0.7943 ± 0.1335   0.7189 ± 0.1848  0.4699 ± 0.2752  0.4294 ± 0.2561  0.6046 ± 0.3480\n",
      "\n",
      "\n",
      "=== ANALYZING BY PROCESS ===\n",
      "\n",
      "Processing process: 2\n",
      "\n",
      "Processing process: 4\n",
      "\n",
      "Processing process: 6\n",
      "\n",
      "Processing process: 8\n",
      "\n",
      "Processing process: 10\n",
      "\n",
      "Processing process: 12\n",
      "\n",
      "Processing process: 14\n",
      "\n",
      "Processing process: 16\n",
      "\n",
      "Processing process: 18\n",
      "\n",
      "\n",
      "=== CREATING VISUALIZATIONS ===\n",
      "Creating error type visualizations...\n",
      "Creating process visualizations...\n",
      "Creating process-wise method ranking visualization...\n",
      "Creating process difficulty analysis...\n",
      "Creating error type performance within processes...\n",
      "Creating summary statistics...\n",
      "Creating F1-score summary table...\n",
      "\n",
      "F1-Score Summary Table (Methods vs Processes):\n",
      "Process        2      4      6      8      10     12     14     16     18\n",
      "Method                                                                   \n",
      "D2C        0.6999 0.7299 0.3746 0.4640 0.4228 0.8812 0.6549 0.4722 0.9757\n",
      "DYNOTEARS  0.0000 0.5708 0.0000 0.0000 0.1960 0.0696 0.0000 0.0000 0.1026\n",
      "Granger    0.0831 0.1663 0.0464 0.0548 0.0341 0.0692 0.0410 0.0072 0.0447\n",
      "MVGC       0.7143 0.8126 0.1668 0.3800 0.0623 0.6752 0.7406 0.0884 0.7577\n",
      "PCMCI      0.7109 0.7894 0.1584 0.3782 0.0637 0.6860 0.8039 0.0603 0.8123\n",
      "PCMCI-GPDC 0.6905 0.7782 0.2614 0.2860 0.0460 0.6187 0.7973 0.0491 0.8299\n",
      "VAR        0.2295 0.2401 0.0236 0.0588 0.0683 0.1772 0.2294 0.0130 0.2595\n",
      "VARLiNGAM  0.5830 0.7089 0.1181 0.3394 0.1840 0.5565 0.7285 0.0465 0.7863\n",
      "Creating comprehensive summary visualization...\n",
      "\n",
      "Analysis complete! Results saved to TEST_analysis/\n",
      "- Tables saved to TEST_analysis/tables/\n",
      "- Error type figures saved to TEST_analysis/figures/error_type/\n",
      "- Process figures saved to TEST_analysis/figures/process/\n",
      "- Summary figure saved to TEST_analysis/figures/\n",
      "\n",
      "=== SUMMARY ===\n",
      "\n",
      "Error Type Summary:\n",
      "gaussian: 360 runs, best method: D2C (F1: 0.6104)\n",
      "uniform: 360 runs, best method: D2C (F1: 0.5852)\n",
      "laplace: 360 runs, best method: D2C (F1: 0.6391)\n",
      "\n",
      "Process Summary:\n",
      "Process 2: 120 runs, best method: PCMCI (F1: 0.7145)\n",
      "Process 4: 120 runs, best method: MVGC (F1: 0.7973)\n",
      "Process 6: 120 runs, best method: D2C (F1: 0.3694)\n",
      "Process 8: 120 runs, best method: D2C (F1: 0.4600)\n",
      "Process 10: 120 runs, best method: D2C (F1: 0.4139)\n",
      "Process 12: 120 runs, best method: D2C (F1: 0.8643)\n",
      "Process 14: 120 runs, best method: PCMCI (F1: 0.8006)\n",
      "Process 16: 120 runs, best method: D2C (F1: 0.4659)\n",
      "Process 18: 120 runs, best method: D2C (F1: 0.9707)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "\n",
    "def analyze_test_dataset(test_pickle_path='data/causal_dfs/causal_dfs_TEST.pkl', output_dir='TEST_analysis'):\n",
    "    \"\"\"\n",
    "    Analyzes the TEST dataset by computing metrics for each error type and process.\n",
    "    \n",
    "    Args:\n",
    "        test_pickle_path (str): Path to the TEST pickle file\n",
    "        output_dir (str): Directory to save results\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all computed metrics and dataframes\n",
    "    \"\"\"\n",
    "    THRESHOLD = 0.309  # Threshold for binary classification\n",
    "    # Configuration\n",
    "    TUPLE_INDEX_TO_METHOD = [\n",
    "        'VAR', 'VARLiNGAM', 'PCMCI', 'MVGC', 'PCMCI-GPDC',\n",
    "        'Granger', 'DYNOTEARS', 'D2C'\n",
    "    ]\n",
    "    # Error type and process mapping (based on your generation code)\n",
    "    error_process_map = {\n",
    "        'gaussian': [2,4,6,8,10,12,14,16,18], \n",
    "        'uniform': [2,4,6,8,10,12,14,16,18], \n",
    "        'laplace': [2,4,6,8,10,12,14,16,18]\n",
    "    }\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(f'{output_dir}/tables', exist_ok=True)\n",
    "    os.makedirs(f'{output_dir}/figures', exist_ok=True)\n",
    "    os.makedirs(f'{output_dir}/figures/error_type', exist_ok=True)\n",
    "    os.makedirs(f'{output_dir}/figures/process', exist_ok=True)\n",
    "    \n",
    "    # Load the TEST data\n",
    "    print(f\"Loading TEST dataset from {test_pickle_path}\")\n",
    "    with open(test_pickle_path, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "    \n",
    "    method_results_tuple = loaded_data[:-1]\n",
    "    true_causal_dfs_dict = loaded_data[-1]\n",
    "    \n",
    "    # Get all run IDs\n",
    "    all_dicts = [d for d in method_results_tuple if d is not None] + [true_causal_dfs_dict]\n",
    "    common_keys = set(all_dicts[0].keys())\n",
    "    for d in all_dicts[1:]:\n",
    "        common_keys.intersection_update(d.keys())\n",
    "    run_ids = sorted(list(common_keys))\n",
    "    \n",
    "    print(f\"Found {len(run_ids)} runs in the dataset\")\n",
    "    \n",
    "    # Map run IDs to error types and processes\n",
    "    def map_run_to_error_and_process(run_id):\n",
    "        \"\"\"Map run ID to error type and process number based on generation order.\"\"\"\n",
    "        # Assuming runs are generated in order: gaussian processes, then uniform, then laplace\n",
    "        # Each error type has 40 time series per process, and 9 processes\n",
    "        total_per_error = 40 * 9  # 360 runs per error type\n",
    "        \n",
    "        if run_id < total_per_error:\n",
    "            error_type = 'gaussian'\n",
    "            process_idx = run_id // 40\n",
    "            process_num = error_process_map['gaussian'][process_idx]\n",
    "        elif run_id < 2 * total_per_error:\n",
    "            error_type = 'uniform'\n",
    "            process_idx = (run_id - total_per_error) // 40\n",
    "            process_num = error_process_map['uniform'][process_idx]\n",
    "        else:\n",
    "            error_type = 'laplace'\n",
    "            process_idx = (run_id - 2 * total_per_error) // 40\n",
    "            process_num = error_process_map['laplace'][process_idx]\n",
    "        \n",
    "        return error_type, process_num\n",
    "    \n",
    "    # Create mapping dictionaries\n",
    "    run_to_error = {}\n",
    "    run_to_process = {}\n",
    "    run_to_error_process = {}\n",
    "    \n",
    "    for run_id in run_ids:\n",
    "        error_type, process_num = map_run_to_error_and_process(run_id)\n",
    "        run_to_error[run_id] = error_type\n",
    "        run_to_process[run_id] = process_num\n",
    "        run_to_error_process[run_id] = f\"{error_type}_process_{process_num}\"\n",
    "    \n",
    "    # Compute metrics function\n",
    "    def compute_metrics(y_true, y_pred):\n",
    "        \"\"\"Compute all metrics for given true and predicted values.\"\"\"\n",
    "        metrics = {\n",
    "            \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "            \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "            \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "            \"F1-Score\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = {\n",
    "        'error_type_results': {},\n",
    "        'process_results': {},\n",
    "        'error_process_results': {},\n",
    "        'detailed_results': []\n",
    "    }\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 1. ANALYSIS BY ERROR TYPE\n",
    "    # =============================================================================\n",
    "    print(\"\\n=== ANALYZING BY ERROR TYPE ===\")\n",
    "    \n",
    "    for error_type in ['gaussian', 'uniform', 'laplace']:\n",
    "        print(f\"\\nProcessing error type: {error_type}\")\n",
    "        \n",
    "        # Get runs for this error type\n",
    "        error_runs = [run_id for run_id in run_ids if run_to_error[run_id] == error_type]\n",
    "        \n",
    "        # Micro-averaging (pooled)\n",
    "        pooled_method_dfs = {}\n",
    "        for i, method_dfs_dict in enumerate(method_results_tuple):\n",
    "            method_name = TUPLE_INDEX_TO_METHOD[i]\n",
    "            if method_dfs_dict is None: continue\n",
    "            \n",
    "            error_dfs = [method_dfs_dict[run_id] for run_id in error_runs if run_id in method_dfs_dict]\n",
    "            if error_dfs:\n",
    "                pooled_method_dfs[method_name] = pd.concat(error_dfs).reset_index(drop=True)\n",
    "        \n",
    "        # True labels for this error type\n",
    "        true_dfs = [true_causal_dfs_dict[run_id] for run_id in error_runs if run_id in true_causal_dfs_dict]\n",
    "        y_true_pooled = pd.concat(true_dfs)['is_causal'].astype(int).reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        # Compute micro-averaged metrics\n",
    "        micro_scores = []\n",
    "        for method_name, pred_df in pooled_method_dfs.items():\n",
    "            if method_name == 'D2C':\n",
    "                y_proba = pred_df['probability'].astype(float)\n",
    "                y_pred = (y_proba > THRESHOLD).astype(int)\n",
    "            else:\n",
    "                y_pred = pred_df['is_causal'].astype(int)\n",
    "\n",
    "            metrics = compute_metrics(y_true_pooled, y_pred)\n",
    "            metrics['Method'] = method_name\n",
    "            micro_scores.append(metrics)\n",
    "        \n",
    "        micro_df = pd.DataFrame(micro_scores).set_index('Method')\n",
    "        \n",
    "        # Macro-averaging (per-run)\n",
    "        macro_results = []\n",
    "        for run_id in error_runs:\n",
    "            y_true_run = true_causal_dfs_dict[run_id]['is_causal'].astype(int)\n",
    "\n",
    "            for i, method_dfs_dict in enumerate(method_results_tuple):\n",
    "                method_name = TUPLE_INDEX_TO_METHOD[i]\n",
    "                if method_dfs_dict is None or run_id not in method_dfs_dict: continue\n",
    "                \n",
    "                if method_name == 'D2C':\n",
    "                    y_proba_run = method_dfs_dict[run_id]['probability'].astype(float)\n",
    "                    y_pred_run = (y_proba_run > THRESHOLD).astype(int)\n",
    "                else:\n",
    "                    y_pred_run = method_dfs_dict[run_id]['is_causal'].astype(int)\n",
    "                \n",
    "                metrics = compute_metrics(y_true_run, y_pred_run)\n",
    "                \n",
    "                for metric_name, score in metrics.items():\n",
    "                    macro_results.append({\n",
    "                        'Method': method_name,\n",
    "                        'Metric': metric_name,\n",
    "                        'Score': score,\n",
    "                        'Run_ID': run_id\n",
    "                    })\n",
    "        \n",
    "        macro_df = pd.DataFrame(macro_results).dropna(subset=['Score'])\n",
    "        macro_summary = macro_df.groupby(['Method', 'Metric'])['Score'].agg(['mean', 'std']).fillna(0)\n",
    "        macro_summary['Mean ± Std'] = macro_summary.apply(\n",
    "            lambda row: f\"{row['mean']:.4f} ± {row['std']:.4f}\", axis=1\n",
    "        )\n",
    "        macro_pivoted = macro_summary['Mean ± Std'].unstack(level='Metric')\n",
    "        \n",
    "        # Store results\n",
    "        results['error_type_results'][error_type] = {\n",
    "            'micro_scores': micro_df,\n",
    "            'macro_summary': macro_pivoted,\n",
    "            'macro_detailed': macro_df\n",
    "        }\n",
    "        \n",
    "        # Save tables\n",
    "        micro_df.to_csv(f\"{output_dir}/tables/micro_{error_type}.csv\", float_format=\"%.4f\")\n",
    "        macro_pivoted.to_csv(f\"{output_dir}/tables/macro_{error_type}.csv\")\n",
    "        \n",
    "        print(f\"Micro-averaged results for {error_type}:\")\n",
    "        print(micro_df.to_string(float_format=\"%.4f\"))\n",
    "        print(f\"\\nMacro-averaged results for {error_type}:\")\n",
    "        print(macro_pivoted.to_string())\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 2. ANALYSIS BY PROCESS\n",
    "    # =============================================================================\n",
    "    print(\"\\n\\n=== ANALYZING BY PROCESS ===\")\n",
    "    \n",
    "    all_processes = sorted(set(run_to_process.values()))\n",
    "    \n",
    "    for process_num in all_processes:\n",
    "        print(f\"\\nProcessing process: {process_num}\")\n",
    "        \n",
    "        # Get runs for this process (across all error types)\n",
    "        process_runs = [run_id for run_id in run_ids if run_to_process[run_id] == process_num]\n",
    "        \n",
    "        # Similar analysis as for error types\n",
    "        pooled_method_dfs = {}\n",
    "        for i, method_dfs_dict in enumerate(method_results_tuple):\n",
    "            method_name = TUPLE_INDEX_TO_METHOD[i]\n",
    "            if method_dfs_dict is None: continue\n",
    "            \n",
    "            process_dfs = [method_dfs_dict[run_id] for run_id in process_runs if run_id in method_dfs_dict]\n",
    "            if process_dfs:\n",
    "                pooled_method_dfs[method_name] = pd.concat(process_dfs).reset_index(drop=True)\n",
    "        \n",
    "        # True labels for this process\n",
    "        true_dfs = [true_causal_dfs_dict[run_id] for run_id in process_runs if run_id in true_causal_dfs_dict]\n",
    "        y_true_pooled = pd.concat(true_dfs)['is_causal'].astype(int).reset_index(drop=True)\n",
    "        # Compute micro-averaged metrics\n",
    "        micro_scores = []\n",
    "        for method_name, pred_df in pooled_method_dfs.items():\n",
    "            if method_name == 'D2C':\n",
    "                y_proba = pred_df['probability'].astype(float)\n",
    "                y_pred = (y_proba > THRESHOLD).astype(int)\n",
    "            else:\n",
    "                y_pred = pred_df['is_causal'].astype(int)\n",
    "            \n",
    "            metrics = compute_metrics(y_true_pooled, y_pred)\n",
    "            metrics['Method'] = method_name\n",
    "            micro_scores.append(metrics)\n",
    "        \n",
    "        micro_df = pd.DataFrame(micro_scores).set_index('Method')\n",
    "        \n",
    "        # Macro-averaging\n",
    "        macro_results = []\n",
    "        for run_id in process_runs:\n",
    "            y_true_run = true_causal_dfs_dict[run_id]['is_causal'].astype(int)\n",
    "\n",
    "            for i, method_dfs_dict in enumerate(method_results_tuple):\n",
    "                method_name = TUPLE_INDEX_TO_METHOD[i]\n",
    "                if method_dfs_dict is None or run_id not in method_dfs_dict: continue\n",
    "                \n",
    "                if method_name == 'D2C':\n",
    "                    y_proba_run = method_dfs_dict[run_id]['probability'].astype(float)\n",
    "                    y_pred_run = (y_proba_run > THRESHOLD).astype(int)\n",
    "                else:\n",
    "                    y_pred_run = method_dfs_dict[run_id]['is_causal'].astype(int)\n",
    "                \n",
    "                metrics = compute_metrics(y_true_run, y_pred_run)\n",
    "                \n",
    "                for metric_name, score in metrics.items():\n",
    "                    macro_results.append({\n",
    "                        'Method': method_name,\n",
    "                        'Metric': metric_name,\n",
    "                        'Score': score,\n",
    "                        'Run_ID': run_id,\n",
    "                        'Error_Type': run_to_error[run_id]\n",
    "                    })\n",
    "        \n",
    "        macro_df = pd.DataFrame(macro_results).dropna(subset=['Score'])\n",
    "        macro_summary = macro_df.groupby(['Method', 'Metric'])['Score'].agg(['mean', 'std']).fillna(0)\n",
    "        macro_summary['Mean ± Std'] = macro_summary.apply(\n",
    "            lambda row: f\"{row['mean']:.4f} ± {row['std']:.4f}\", axis=1\n",
    "        )\n",
    "        macro_pivoted = macro_summary['Mean ± Std'].unstack(level='Metric')\n",
    "        \n",
    "        # Store results\n",
    "        results['process_results'][process_num] = {\n",
    "            'micro_scores': micro_df,\n",
    "            'macro_summary': macro_pivoted,\n",
    "            'macro_detailed': macro_df\n",
    "        }\n",
    "        \n",
    "        # Save tables\n",
    "        micro_df.to_csv(f\"{output_dir}/tables/micro_process_{process_num}.csv\", float_format=\"%.4f\")\n",
    "        macro_pivoted.to_csv(f\"{output_dir}/tables/macro_process_{process_num}.csv\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 3. CREATE SUMMARY VISUALIZATIONS\n",
    "    # =============================================================================\n",
    "    print(\"\\n\\n=== CREATING VISUALIZATIONS ===\")\n",
    "    \n",
    "    # Set plotting style\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 3.1 ERROR TYPE VISUALIZATIONS\n",
    "    # =============================================================================\n",
    "    print(\"Creating error type visualizations...\")\n",
    "    \n",
    "    # Collect all macro data for error type comparison\n",
    "    all_error_data = []\n",
    "    for error_type, data in results['error_type_results'].items():\n",
    "        df = data['macro_detailed'].copy()\n",
    "        df['Error_Type'] = error_type\n",
    "        all_error_data.append(df)\n",
    "    \n",
    "    if all_error_data:\n",
    "        combined_error_df = pd.concat(all_error_data)\n",
    "        \n",
    "        # Create boxplot for each metric\n",
    "        metrics = combined_error_df['Metric'].unique()\n",
    "        for metric in metrics:\n",
    "            metric_data = combined_error_df[combined_error_df['Metric'] == metric]\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(14, 8))\n",
    "            sns.boxplot(data=metric_data, x='Error_Type', y='Score', hue='Method', ax=ax, palette='Set3')\n",
    "            ax.set_title(f'{metric} by Error Type', fontsize=16, pad=20)\n",
    "            ax.set_xlabel('Error Type', fontsize=14)\n",
    "            ax.set_ylabel(f'{metric} Score', fontsize=14)\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            fig.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/figures/error_type/boxplot_{metric}_by_error_type.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 3.2 PROCESS VISUALIZATIONS\n",
    "    # =============================================================================\n",
    "    print(\"Creating process visualizations...\")\n",
    "    \n",
    "    # Collect all macro data for process comparison\n",
    "    all_process_data = []\n",
    "    for process_num, data in results['process_results'].items():\n",
    "        df = data['macro_detailed'].copy()\n",
    "        df['Process'] = process_num\n",
    "        all_process_data.append(df)\n",
    "    \n",
    "    if all_process_data:\n",
    "        combined_process_df = pd.concat(all_process_data)\n",
    "\n",
    "        # --- Create Overall Summary Table (Macro-Averaged) ---\n",
    "        print(\"\\n--- Creating Overall Summary Table (Macro-Averaged) ---\")\n",
    "        \n",
    "        # Use the combined_process_df which contains all per-run scores\n",
    "        overall_summary = combined_process_df.groupby(['Method', 'Metric'])['Score'].agg(['mean', 'std']).fillna(0)\n",
    "        overall_summary['Mean ± Std'] = overall_summary.apply(\n",
    "            lambda row: f\"{row['mean']:.4f} ± {row['std']:.4f}\", axis=1\n",
    "        )\n",
    "        overall_pivoted_summary = overall_summary['Mean ± Std'].unstack(level='Metric')\n",
    "\n",
    "        # Store results for later access\n",
    "        results['overall_summary'] = overall_pivoted_summary\n",
    "\n",
    "        # Save the summary table to a CSV file\n",
    "        overall_pivoted_summary.to_csv(f\"{output_dir}/tables/overall_macro_summary.csv\")\n",
    "\n",
    "        print(\"Overall Macro-Averaged Results (Mean ± Std across all runs):\")\n",
    "        print(overall_pivoted_summary.to_string())\n",
    "        print(\"----------------------------------------------------------\\n\")\n",
    "        \n",
    "        # Create boxplots for each metric by process\n",
    "        metrics = combined_process_df['Metric'].unique()\n",
    "        for metric in metrics:\n",
    "            metric_data = combined_process_df[combined_process_df['Metric'] == metric]\n",
    "            \n",
    "            # Boxplot by process\n",
    "            fig, ax = plt.subplots(figsize=(16, 8))\n",
    "            sns.boxplot(data=metric_data, x='Process', y='Score', hue='Method', ax=ax, palette='Set3')\n",
    "            ax.set_title(f'{metric} by Process Number', fontsize=16, pad=20)\n",
    "            ax.set_xlabel('Process Number', fontsize=14)\n",
    "            ax.set_ylabel(f'{metric} Score', fontsize=14)\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            # Rotate x-axis labels for better readability\n",
    "            plt.xticks(rotation=45)\n",
    "            fig.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/figures/process/boxplot_{metric}_by_process.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Heatmap showing method performance across processes\n",
    "            pivot_data = metric_data.groupby(['Process', 'Method'])['Score'].mean().unstack()\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            sns.heatmap(pivot_data.T, annot=True, fmt='.3f', cmap='RdYlBu_r', ax=ax, \n",
    "                       cbar_kws={'label': f'{metric} Score'})\n",
    "            ax.set_title(f'{metric} Heatmap: Methods vs Processes', fontsize=16, pad=20)\n",
    "            ax.set_xlabel('Process Number', fontsize=14)\n",
    "            ax.set_ylabel('Method', fontsize=14)\n",
    "            fig.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/figures/process/heatmap_{metric}_by_process.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        # Create violin plots showing distribution differences across processes\n",
    "        for metric in metrics:\n",
    "            metric_data = combined_process_df[combined_process_df['Metric'] == metric]\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(16, 8))\n",
    "            sns.violinplot(data=metric_data, x='Process', y='Score', hue='Method', ax=ax, palette='Set3')\n",
    "            ax.set_title(f'{metric} Distribution by Process Number', fontsize=16, pad=20)\n",
    "            ax.set_xlabel('Process Number', fontsize=14)\n",
    "            ax.set_ylabel(f'{metric} Score', fontsize=14)\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.xticks(rotation=45)\n",
    "            fig.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/figures/process/violin_{metric}_by_process.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        # Process-wise method ranking visualization\n",
    "        print(\"Creating process-wise method ranking visualization...\")\n",
    "        \n",
    "        # Calculate average F1-score for each method across all processes\n",
    "        f1_data = combined_process_df[combined_process_df['Metric'] == 'F1-Score']\n",
    "        process_method_f1 = f1_data.groupby(['Process', 'Method'])['Score'].mean().unstack()\n",
    "        \n",
    "        # Rank methods for each process (1 = best)\n",
    "        process_rankings = process_method_f1.rank(axis=1, ascending=False)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        sns.heatmap(process_rankings.T, annot=True, fmt='.0f', cmap='RdYlGn_r', ax=ax,\n",
    "                   cbar_kws={'label': 'Rank (1=Best)'})\n",
    "        ax.set_title('Method Rankings by Process (Based on F1-Score)', fontsize=16, pad=20)\n",
    "        ax.set_xlabel('Process Number', fontsize=14)\n",
    "        ax.set_ylabel('Method', fontsize=14)\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/figures/process/method_rankings_by_process.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Process difficulty analysis (based on overall performance)\n",
    "        print(\"Creating process difficulty analysis...\")\n",
    "        \n",
    "        process_difficulty = f1_data.groupby('Process')['Score'].agg(['mean', 'std'])\n",
    "        process_difficulty['difficulty'] = 1 - process_difficulty['mean']  # Higher difficulty = lower average F1\n",
    "        process_difficulty = process_difficulty.sort_values('difficulty', ascending=False)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Bar plot of average F1-score by process\n",
    "        ax1.bar(range(len(process_difficulty)), process_difficulty['mean'], \n",
    "                yerr=process_difficulty['std'], capsize=5, alpha=0.7, color='skyblue')\n",
    "        ax1.set_xlabel('Process Number', fontsize=12)\n",
    "        ax1.set_ylabel('Average F1-Score', fontsize=12)\n",
    "        ax1.set_title('Average Method Performance by Process', fontsize=14)\n",
    "        ax1.set_xticks(range(len(process_difficulty)))\n",
    "        ax1.set_xticklabels(process_difficulty.index, rotation=45)\n",
    "        \n",
    "        # Process difficulty ranking\n",
    "        ax2.barh(range(len(process_difficulty)), process_difficulty['difficulty'], alpha=0.7, color='coral')\n",
    "        ax2.set_ylabel('Process Number', fontsize=12)\n",
    "        ax2.set_xlabel('Difficulty Score (1 - Avg F1)', fontsize=12)\n",
    "        ax2.set_title('Process Difficulty Ranking', fontsize=14)\n",
    "        ax2.set_yticks(range(len(process_difficulty)))\n",
    "        ax2.set_yticklabels(process_difficulty.index)\n",
    "        ax2.invert_yaxis()\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/figures/process/process_difficulty_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Error type performance within each process\n",
    "        print(\"Creating error type performance within processes...\")\n",
    "        \n",
    "        for metric in ['F1-Score', 'Accuracy']:\n",
    "            metric_data = combined_process_df[combined_process_df['Metric'] == metric]\n",
    "            \n",
    "            # Create a summary plot showing error type performance within each process\n",
    "            fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for i, process_num in enumerate(sorted(all_processes)):\n",
    "                if i < len(axes):\n",
    "                    process_data = metric_data[metric_data['Process'] == process_num]\n",
    "                    \n",
    "                    if not process_data.empty:\n",
    "                        sns.boxplot(data=process_data, x='Error_Type', y='Score', hue='Method', \n",
    "                                   ax=axes[i], palette='Set3')\n",
    "                        axes[i].set_title(f'Process {process_num}', fontsize=12)\n",
    "                        axes[i].set_xlabel('Error Type', fontsize=10)\n",
    "                        axes[i].set_ylabel(f'{metric}', fontsize=10)\n",
    "                        axes[i].legend().set_visible(False)  # Hide individual legends\n",
    "            \n",
    "            # Remove empty subplots\n",
    "            for j in range(len(all_processes), len(axes)):\n",
    "                fig.delaxes(axes[j])\n",
    "            \n",
    "            # Add a single legend for all subplots\n",
    "            handles, labels = axes[0].get_legend_handles_labels()\n",
    "            fig.legend(handles, labels, loc='center right', bbox_to_anchor=(0.98, 0.5))\n",
    "            \n",
    "            fig.suptitle(f'{metric} by Error Type within Each Process', fontsize=16, y=0.98)\n",
    "            fig.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/figures/process/error_type_within_process_{metric.lower().replace('-', '_')}.png\", \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 3.3 SUMMARY STATISTICS AND COMBINED VISUALIZATIONS\n",
    "    # =============================================================================\n",
    "    print(\"Creating summary statistics...\")\n",
    "    \n",
    "    # Summary statistics across error types and processes\n",
    "    summary_stats = {\n",
    "        'error_type_summary': {},\n",
    "        'process_summary': {}\n",
    "    }\n",
    "    \n",
    "    # Error type summary\n",
    "    for error_type, data in results['error_type_results'].items():\n",
    "        summary_stats['error_type_summary'][error_type] = {\n",
    "            'n_runs': len(data['macro_detailed']['Run_ID'].unique()),\n",
    "            'best_method_f1': data['micro_scores']['F1-Score'].idxmax(),\n",
    "            'best_f1_score': data['micro_scores']['F1-Score'].max()\n",
    "        }\n",
    "    \n",
    "    # Process summary  \n",
    "    for process_num, data in results['process_results'].items():\n",
    "        summary_stats['process_summary'][process_num] = {\n",
    "            'n_runs': len(data['macro_detailed']['Run_ID'].unique()),\n",
    "            'best_method_f1': data['micro_scores']['F1-Score'].idxmax(),\n",
    "            'best_f1_score': data['micro_scores']['F1-Score'].max()\n",
    "        }\n",
    "    \n",
    "    results['summary_stats'] = summary_stats\n",
    "    \n",
    "    print(\"Creating F1-score summary table...\")\n",
    "\n",
    "    if all_process_data:\n",
    "        f1_process_data = combined_process_df[combined_process_df['Metric'] == 'F1-Score']\n",
    "        f1_summary_table = f1_process_data.groupby(['Method', 'Process'])['Score'].mean().unstack()\n",
    "        \n",
    "        # Save the table\n",
    "        f1_summary_table.to_csv(f\"{output_dir}/tables/f1_summary_methods_vs_processes.csv\", float_format=\"%.4f\")\n",
    "        \n",
    "        print(\"\\nF1-Score Summary Table (Methods vs Processes):\")\n",
    "        print(f1_summary_table.to_string(float_format=\"%.4f\"))\n",
    "        \n",
    "        # Add to results for later access\n",
    "        results['f1_summary_table'] = f1_summary_table\n",
    "\n",
    "    # Create a comprehensive summary visualization\n",
    "    print(\"Creating comprehensive summary visualization...\")\n",
    "    \n",
    "    if all_error_data and all_process_data:\n",
    "        # Combined overview plot\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
    "        \n",
    "        # 1. Error type performance overview\n",
    "        f1_error_data = combined_error_df[combined_error_df['Metric'] == 'F1-Score']\n",
    "        error_summary = f1_error_data.groupby(['Error_Type', 'Method'])['Score'].mean().unstack()\n",
    "        sns.heatmap(error_summary.T, annot=True, fmt='.3f', cmap='RdYlBu_r', ax=ax1)\n",
    "        ax1.set_title('F1-Score by Error Type', fontsize=14)\n",
    "        ax1.set_xlabel('Error Type', fontsize=12)\n",
    "        ax1.set_ylabel('Method', fontsize=12)\n",
    "        \n",
    "        # 2. Process performance overview (top methods only for readability)\n",
    "        f1_process_data = combined_process_df[combined_process_df['Metric'] == 'F1-Score']\n",
    "        process_summary = f1_process_data.groupby(['Process', 'Method'])['Score'].mean().unstack()\n",
    "        # Select top 4 methods based on overall performance\n",
    "        method_means = process_summary.mean(axis=0).sort_values(ascending=False)\n",
    "        top_methods = method_means.head(4).index\n",
    "        sns.heatmap(process_summary[top_methods].T, annot=True, fmt='.3f', cmap='RdYlBu_r', ax=ax2)\n",
    "        ax2.set_title('F1-Score by Process (Top 4 Methods)', fontsize=14)\n",
    "        ax2.set_xlabel('Process Number', fontsize=12)\n",
    "        ax2.set_ylabel('Method', fontsize=12)\n",
    "        \n",
    "        # 3. Method consistency across error types\n",
    "        error_std = f1_error_data.groupby(['Method', 'Error_Type'])['Score'].std().unstack()\n",
    "        sns.heatmap(error_std, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax3)\n",
    "        ax3.set_title('F1-Score Standard Deviation by Error Type', fontsize=14)\n",
    "        ax3.set_xlabel('Error Type', fontsize=12)\n",
    "        ax3.set_ylabel('Method', fontsize=12)\n",
    "        \n",
    "        # 4. Method consistency across processes\n",
    "        process_std = f1_process_data.groupby('Method')['Score'].std().sort_values()\n",
    "        ax4.barh(range(len(process_std)), process_std.values, alpha=0.7)\n",
    "        ax4.set_yticks(range(len(process_std)))\n",
    "        ax4.set_yticklabels(process_std.index)\n",
    "        ax4.set_xlabel('F1-Score Standard Deviation', fontsize=12)\n",
    "        ax4.set_title('Method Consistency Across All Processes', fontsize=14)\n",
    "        \n",
    "        fig.suptitle('Comprehensive Performance Analysis Summary', fontsize=18, y=0.98)\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/figures/comprehensive_summary.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"\\nAnalysis complete! Results saved to {output_dir}/\")\n",
    "    print(f\"- Tables saved to {output_dir}/tables/\")\n",
    "    print(f\"- Error type figures saved to {output_dir}/figures/error_type/\")\n",
    "    print(f\"- Process figures saved to {output_dir}/figures/process/\")\n",
    "    print(f\"- Summary figure saved to {output_dir}/figures/\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    results = analyze_test_dataset()\n",
    "    \n",
    "    # Print some summary information\n",
    "    print(\"\\n=== SUMMARY ===\")\n",
    "    print(\"\\nError Type Summary:\")\n",
    "    for error_type, stats in results['summary_stats']['error_type_summary'].items():\n",
    "        print(f\"{error_type}: {stats['n_runs']} runs, best method: {stats['best_method_f1']} (F1: {stats['best_f1_score']:.4f})\")\n",
    "    \n",
    "    print(\"\\nProcess Summary:\")\n",
    "    for process_num, stats in results['summary_stats']['process_summary'].items():\n",
    "        print(f\"Process {process_num}: {stats['n_runs']} runs, best method: {stats['best_method_f1']} (F1: {stats['best_f1_score']:.4f})\")\n",
    "\n",
    "    # Create F1-score summary table (Methods vs Processes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "td2c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
