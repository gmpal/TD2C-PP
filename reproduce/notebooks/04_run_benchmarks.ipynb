{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avoids the need for users to install TD2C as a package\n",
    "import sys\n",
    "sys.path.append('../..') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# --- Environment Setup for Performance ---\n",
    "# Limit threads to prevent over-subscription, especially by libraries like MKL used in VARLiNGAM\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "# --- Add Project Source to Path ---\n",
    "# Assumes the script is run from a directory where '../src' is the correct path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# --- Import Custom Libraries ---\n",
    "from d2c.descriptors import DataLoader\n",
    "from d2c.benchmark import VARLiNGAM, PCMCI, Granger, DYNOTEARS, D2CWrapper, VAR, MultivariateGranger\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from d2c.benchmark.utils import prepare_prediction_df_d2c\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_testing_data():\n",
    "    \"\"\"\n",
    "    !!! Special Helper Function for testing data loading. \n",
    "    !!! It requires special treatment because it's split into 3 files because of the noise distributions.\n",
    "    Loads testing data from pickle files for different error distributions.\n",
    "\n",
    "    This function initializes data loaders for three types of error distributions: \n",
    "    'gaussian', 'uniform', and 'laplace'. It retrieves original observations, \n",
    "    lagged flattened observations, flattened directed acyclic graphs (DAGs), \n",
    "    and true causal dataframes from the data loaders. The function aggregates \n",
    "    these observations and returns them as lists.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - original_observations_list_testing (list): A list of original observations from the testing data.\n",
    "            - true_causal_dfs_list_testing (list): A list of true causal dataframes from the testing data.\n",
    "    \"\"\"\n",
    "    dataloaders = {}\n",
    "    original_observations_testing = {}\n",
    "    lagged_flattened_observations_testing = {}\n",
    "    flattened_dags_testing = {}\n",
    "    true_causal_dfs = {}\n",
    "\n",
    "    for error_dist in ['gaussian', 'uniform', 'laplace']:\n",
    "        dataloader = DataLoader(n_variables = 5,\n",
    "                                maxlags = 3)\n",
    "        dataloader.from_pickle(f'data/observations/testing_data_{error_dist}.pkl')\n",
    "        \n",
    "        dataloaders[error_dist] = dataloader\n",
    "        original_observations_testing[error_dist] = dataloader.get_original_observations()\n",
    "        lagged_flattened_observations_testing[error_dist] = dataloader.get_observations()\n",
    "        flattened_dags_testing[error_dist] = dataloader.get_dags()\n",
    "        true_causal_dfs[error_dist] = dataloader.get_true_causal_dfs()\n",
    "\n",
    "    original_observations_list_testing = []\n",
    "    for obs_list in original_observations_testing.values():\n",
    "        original_observations_list_testing.extend(obs_list) \n",
    "\n",
    "    lagged_flattened_observations_list_testing = []\n",
    "    for obs_list in lagged_flattened_observations_testing.values():\n",
    "        lagged_flattened_observations_list_testing.extend(obs_list)\n",
    "\n",
    "    flattened_dags_list_testing = []\n",
    "    for dags_list in flattened_dags_testing.values():\n",
    "        flattened_dags_list_testing.extend(dags_list)\n",
    "\n",
    "    true_causal_dfs_list_testing = []\n",
    "    for causal_df in true_causal_dfs.values():\n",
    "        true_causal_dfs_list_testing.extend(causal_df)\n",
    "\n",
    "    return original_observations_list_testing ,true_causal_dfs_list_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 1. SCRIPT CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "N_JOBS = 50       # Max jobs for parallelizable tasks\n",
    "MAXLAGS = 3       # Max lags for all models\n",
    "DESCRIPTORS_DIR = Path('data/descriptors/')\n",
    "PRE_RESULTS_DIR = Path('data/before_d2c/')\n",
    "RESULTS_DIR = Path('data/causal_dfs/')\n",
    "THRESHOLD = 0.309\n",
    "\n",
    "# Ensure directories exist\n",
    "PRE_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define all datasets to be processed in a list of dictionaries\n",
    "DATASETS_TO_PROCESS = [\n",
    "    {\n",
    "        \"name\": \"DREAM3_10\",\n",
    "        \"n_vars\": 10,\n",
    "        \"input_file\": \"data/realistic/dream3/dream3_10.pkl\",\n",
    "        \"d2c_descriptors_file\": \"descriptors_dream3_10.pkl\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DREAM3_50\",\n",
    "        \"n_vars\": 50,\n",
    "        \"input_file\": \"data/realistic/dream3/dream3_50.pkl\",\n",
    "        \"d2c_descriptors_file\": \"descriptors_dream3_50.pkl\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"NETSIM_5\",\n",
    "        \"n_vars\": 5,\n",
    "        \"input_file\": \"data/realistic/netsym/netsym_5.pkl\",\n",
    "        \"d2c_descriptors_file\": \"descriptors_netsim_5.pkl\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"NETSIM_10\",\n",
    "        \"n_vars\": 10,\n",
    "        \"input_file\": \"data/realistic/netsym/netsym_10.pkl\",\n",
    "        \"d2c_descriptors_file\": \"descriptors_netsim_10.pkl\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"TEST\",\n",
    "        \"n_vars\": 5,\n",
    "        \"input_file\": None, #requires special treatment\n",
    "        \"d2c_descriptors_file\": \"descriptors_df_test.pkl\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "--- Training D2C Classifier (once for all benchmarks) ---\n",
      "Loading training data from: data/descriptors/descriptors_df_train.pkl\n",
      "Fitting the classifier...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier training complete.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# --- 2. TRAIN THE TD2C Classifier (ONCE) ---\n",
    "# ==============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"--- Training D2C Classifier (once for all benchmarks) ---\")\n",
    "\n",
    "# Use the 'no_copula' version of the training data as per our decision\n",
    "training_data_path = DESCRIPTORS_DIR / 'descriptors_df_train.pkl'\n",
    "print(f\"Loading training data from: {training_data_path}\")\n",
    "\n",
    "try:\n",
    "    descriptors_df_train = pd.read_pickle(training_data_path)\n",
    "    # Using 0 imputation for cases when MB members don't exist (childless/parentless nodes)\n",
    "    descriptors_df_train.fillna(0, inplace=True)\n",
    "\n",
    "    X_train = descriptors_df_train.drop(columns=['graph_id', 'edge_source', 'edge_dest', 'is_causal'])\n",
    "    y_train = descriptors_df_train['is_causal']\n",
    "\n",
    "    # Define the classifier\n",
    "    clf = BalancedRandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        sampling_strategy='auto',\n",
    "        replacement=True,\n",
    "        bootstrap=True,\n",
    "        n_jobs=N_JOBS\n",
    "    )\n",
    "    \n",
    "    print(\"Fitting the classifier...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Classifier training complete.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: Training file not found at {training_data_path}. Cannot proceed.\")\n",
    "    sys.exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- PROCESSING DATASET: DREAM3_10 ---\n",
      "Variables: 10, Max Lags: 3\n",
      "Found cached competitor results. Loading from: data/before_d2c/causal_dfs_before_d2c_DREAM3_10.pkl\n",
      "Running D2C for DREAM3_10...\n",
      "  - Using precomputed descriptors from: data/descriptors/descriptors_dream3_10.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC BEFORE: 0.7732920450357789\n",
      "Loading and pre-computing predictions from: data/descriptors/descriptors_dream3_10.pkl\n",
      "--- DEBUGGING D2CWRAPPER ---\n",
      "Initial descriptor columns (67): ['HOC_1_2', 'HOC_1_3', 'HOC_2_1', 'HOC_3_1', 'cau_eff', 'cau_eff_mbcau', 'cau_eff_mbeff_plus_child', 'cau_eff_mbeff_plus_interaction', 'cau_eff_mbeff_plus_mean', 'cau_eff_mbeff_plus_parent', 'cau_eff_mbeff_plus_std', 'cau_m_eff_interaction', 'cau_m_eff_mean', 'cau_m_eff_std', 'coeff_cause', 'coeff_eff', 'com_cau', 'edge_dest', 'edge_source', 'eff_cau', 'eff_cau_mbcau_plus_interaction', 'eff_cau_mbcau_plus_mean', 'eff_cau_mbcau_plus_std', 'eff_cau_mbeff', 'eff_m_cau_child', 'eff_m_cau_interaction', 'eff_m_cau_mean', 'eff_m_cau_parent', 'eff_m_cau_std', 'errors_correlation_with_inputs', 'graph_id', 'is_causal', 'kurtosis_ca', 'kurtosis_ef', 'm_cau_interaction', 'm_cau_mean', 'm_cau_std', 'm_eff_child', 'm_eff_interaction', 'm_eff_mean', 'm_eff_parent', 'm_eff_std', 'mbe_mbe_eff_interaction', 'mbe_mbe_eff_mean', 'mbe_mbe_eff_std', 'mca_mca_cau_child', 'mca_mca_cau_interaction', 'mca_mca_cau_mean', 'mca_mca_cau_parent', 'mca_mca_cau_std', 'mca_mef_cau_child', 'mca_mef_cau_interaction', 'mca_mef_cau_mean', 'mca_mef_cau_parent', 'mca_mef_cau_std', 'mca_mef_eff_child', 'mca_mef_eff_interaction', 'mca_mef_eff_mean', 'mca_mef_eff_parent', 'mca_mef_eff_std', 'parcorr_errors', 'skewness_ca', 'skewness_ef', 'te_asymmetry_diff_1_15', 'transfer_entropy_bwd', 'transfer_entropy_diff', 'transfer_entropy_fwd']\n",
      "Model feature names (63): ['HOC_1_2', 'HOC_1_3', 'HOC_2_1', 'HOC_3_1', 'cau_eff', 'cau_eff_mbcau', 'cau_eff_mbeff_plus_child', 'cau_eff_mbeff_plus_interaction', 'cau_eff_mbeff_plus_mean', 'cau_eff_mbeff_plus_parent', 'cau_eff_mbeff_plus_std', 'cau_m_eff_interaction', 'cau_m_eff_mean', 'cau_m_eff_std', 'coeff_cause', 'coeff_eff', 'com_cau', 'eff_cau', 'eff_cau_mbcau_plus_interaction', 'eff_cau_mbcau_plus_mean', 'eff_cau_mbcau_plus_std', 'eff_cau_mbeff', 'eff_m_cau_child', 'eff_m_cau_interaction', 'eff_m_cau_mean', 'eff_m_cau_parent', 'eff_m_cau_std', 'errors_correlation_with_inputs', 'kurtosis_ca', 'kurtosis_ef', 'm_cau_interaction', 'm_cau_mean', 'm_cau_std', 'm_eff_child', 'm_eff_interaction', 'm_eff_mean', 'm_eff_parent', 'm_eff_std', 'mbe_mbe_eff_interaction', 'mbe_mbe_eff_mean', 'mbe_mbe_eff_std', 'mca_mca_cau_child', 'mca_mca_cau_interaction', 'mca_mca_cau_mean', 'mca_mca_cau_parent', 'mca_mca_cau_std', 'mca_mef_cau_child', 'mca_mef_cau_interaction', 'mca_mef_cau_mean', 'mca_mef_cau_parent', 'mca_mef_cau_std', 'mca_mef_eff_child', 'mca_mef_eff_interaction', 'mca_mef_eff_mean', 'mca_mef_eff_parent', 'mca_mef_eff_std', 'parcorr_errors', 'skewness_ca', 'skewness_ef', 'te_asymmetry_diff_1_15', 'transfer_entropy_bwd', 'transfer_entropy_diff', 'transfer_entropy_fwd']\n",
      "Running in single-threaded mode or managing parallelism manually.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff06d5c91d84455e83f976a89ef64c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Time Series:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2C run complete.\n",
      "NaNs found in the final y_true vector: 0\n",
      "ROC AUC AFTER 0.7732920450357789\n",
      "Successfully saved all final results for DREAM3_10 to: data/causal_dfs/causal_dfs_DREAM3_10.pkl\n",
      "\n",
      "============================================================\n",
      "--- PROCESSING DATASET: DREAM3_50 ---\n",
      "Variables: 50, Max Lags: 3\n",
      "Found cached competitor results. Loading from: data/before_d2c/causal_dfs_before_d2c_DREAM3_50.pkl\n",
      "Running D2C for DREAM3_50...\n",
      "  - Using precomputed descriptors from: data/descriptors/descriptors_dream3_50.pkl\n",
      "ROC AUC BEFORE: 0.7915524602028011\n",
      "Loading and pre-computing predictions from: data/descriptors/descriptors_dream3_50.pkl\n",
      "--- DEBUGGING D2CWRAPPER ---\n",
      "Initial descriptor columns (67): ['HOC_1_2', 'HOC_1_3', 'HOC_2_1', 'HOC_3_1', 'cau_eff', 'cau_eff_mbcau', 'cau_eff_mbeff_plus_child', 'cau_eff_mbeff_plus_interaction', 'cau_eff_mbeff_plus_mean', 'cau_eff_mbeff_plus_parent', 'cau_eff_mbeff_plus_std', 'cau_m_eff_interaction', 'cau_m_eff_mean', 'cau_m_eff_std', 'coeff_cause', 'coeff_eff', 'com_cau', 'edge_dest', 'edge_source', 'eff_cau', 'eff_cau_mbcau_plus_interaction', 'eff_cau_mbcau_plus_mean', 'eff_cau_mbcau_plus_std', 'eff_cau_mbeff', 'eff_m_cau_child', 'eff_m_cau_interaction', 'eff_m_cau_mean', 'eff_m_cau_parent', 'eff_m_cau_std', 'errors_correlation_with_inputs', 'graph_id', 'is_causal', 'kurtosis_ca', 'kurtosis_ef', 'm_cau_interaction', 'm_cau_mean', 'm_cau_std', 'm_eff_child', 'm_eff_interaction', 'm_eff_mean', 'm_eff_parent', 'm_eff_std', 'mbe_mbe_eff_interaction', 'mbe_mbe_eff_mean', 'mbe_mbe_eff_std', 'mca_mca_cau_child', 'mca_mca_cau_interaction', 'mca_mca_cau_mean', 'mca_mca_cau_parent', 'mca_mca_cau_std', 'mca_mef_cau_child', 'mca_mef_cau_interaction', 'mca_mef_cau_mean', 'mca_mef_cau_parent', 'mca_mef_cau_std', 'mca_mef_eff_child', 'mca_mef_eff_interaction', 'mca_mef_eff_mean', 'mca_mef_eff_parent', 'mca_mef_eff_std', 'parcorr_errors', 'skewness_ca', 'skewness_ef', 'te_asymmetry_diff_1_15', 'transfer_entropy_bwd', 'transfer_entropy_diff', 'transfer_entropy_fwd']\n",
      "Model feature names (63): ['HOC_1_2', 'HOC_1_3', 'HOC_2_1', 'HOC_3_1', 'cau_eff', 'cau_eff_mbcau', 'cau_eff_mbeff_plus_child', 'cau_eff_mbeff_plus_interaction', 'cau_eff_mbeff_plus_mean', 'cau_eff_mbeff_plus_parent', 'cau_eff_mbeff_plus_std', 'cau_m_eff_interaction', 'cau_m_eff_mean', 'cau_m_eff_std', 'coeff_cause', 'coeff_eff', 'com_cau', 'eff_cau', 'eff_cau_mbcau_plus_interaction', 'eff_cau_mbcau_plus_mean', 'eff_cau_mbcau_plus_std', 'eff_cau_mbeff', 'eff_m_cau_child', 'eff_m_cau_interaction', 'eff_m_cau_mean', 'eff_m_cau_parent', 'eff_m_cau_std', 'errors_correlation_with_inputs', 'kurtosis_ca', 'kurtosis_ef', 'm_cau_interaction', 'm_cau_mean', 'm_cau_std', 'm_eff_child', 'm_eff_interaction', 'm_eff_mean', 'm_eff_parent', 'm_eff_std', 'mbe_mbe_eff_interaction', 'mbe_mbe_eff_mean', 'mbe_mbe_eff_std', 'mca_mca_cau_child', 'mca_mca_cau_interaction', 'mca_mca_cau_mean', 'mca_mca_cau_parent', 'mca_mca_cau_std', 'mca_mef_cau_child', 'mca_mef_cau_interaction', 'mca_mef_cau_mean', 'mca_mef_cau_parent', 'mca_mef_cau_std', 'mca_mef_eff_child', 'mca_mef_eff_interaction', 'mca_mef_eff_mean', 'mca_mef_eff_parent', 'mca_mef_eff_std', 'parcorr_errors', 'skewness_ca', 'skewness_ef', 'te_asymmetry_diff_1_15', 'transfer_entropy_bwd', 'transfer_entropy_diff', 'transfer_entropy_fwd']\n",
      "Running in single-threaded mode or managing parallelism manually.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336f884fd83446b8ad4ec2c8c80e77e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Time Series:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2C run complete.\n",
      "NaNs found in the final y_true vector: 0\n",
      "ROC AUC AFTER 0.7915524602028011\n",
      "Successfully saved all final results for DREAM3_50 to: data/causal_dfs/causal_dfs_DREAM3_50.pkl\n",
      "\n",
      "============================================================\n",
      "--- PROCESSING DATASET: NETSIM_5 ---\n",
      "Variables: 5, Max Lags: 3\n",
      "Found cached competitor results. Loading from: data/before_d2c/causal_dfs_before_d2c_NETSIM_5.pkl\n",
      "Running D2C for NETSIM_5...\n",
      "  - Using precomputed descriptors from: data/descriptors/descriptors_netsim_5.pkl\n",
      "ROC AUC BEFORE: 0.8251527865834245\n",
      "Loading and pre-computing predictions from: data/descriptors/descriptors_netsim_5.pkl\n",
      "--- DEBUGGING D2CWRAPPER ---\n",
      "Initial descriptor columns (67): ['HOC_1_2', 'HOC_1_3', 'HOC_2_1', 'HOC_3_1', 'cau_eff', 'cau_eff_mbcau', 'cau_eff_mbeff_plus_child', 'cau_eff_mbeff_plus_interaction', 'cau_eff_mbeff_plus_mean', 'cau_eff_mbeff_plus_parent', 'cau_eff_mbeff_plus_std', 'cau_m_eff_interaction', 'cau_m_eff_mean', 'cau_m_eff_std', 'coeff_cause', 'coeff_eff', 'com_cau', 'edge_dest', 'edge_source', 'eff_cau', 'eff_cau_mbcau_plus_interaction', 'eff_cau_mbcau_plus_mean', 'eff_cau_mbcau_plus_std', 'eff_cau_mbeff', 'eff_m_cau_child', 'eff_m_cau_interaction', 'eff_m_cau_mean', 'eff_m_cau_parent', 'eff_m_cau_std', 'errors_correlation_with_inputs', 'graph_id', 'is_causal', 'kurtosis_ca', 'kurtosis_ef', 'm_cau_interaction', 'm_cau_mean', 'm_cau_std', 'm_eff_child', 'm_eff_interaction', 'm_eff_mean', 'm_eff_parent', 'm_eff_std', 'mbe_mbe_eff_interaction', 'mbe_mbe_eff_mean', 'mbe_mbe_eff_std', 'mca_mca_cau_child', 'mca_mca_cau_interaction', 'mca_mca_cau_mean', 'mca_mca_cau_parent', 'mca_mca_cau_std', 'mca_mef_cau_child', 'mca_mef_cau_interaction', 'mca_mef_cau_mean', 'mca_mef_cau_parent', 'mca_mef_cau_std', 'mca_mef_eff_child', 'mca_mef_eff_interaction', 'mca_mef_eff_mean', 'mca_mef_eff_parent', 'mca_mef_eff_std', 'parcorr_errors', 'skewness_ca', 'skewness_ef', 'te_asymmetry_diff_1_15', 'transfer_entropy_bwd', 'transfer_entropy_diff', 'transfer_entropy_fwd']\n",
      "Model feature names (63): ['HOC_1_2', 'HOC_1_3', 'HOC_2_1', 'HOC_3_1', 'cau_eff', 'cau_eff_mbcau', 'cau_eff_mbeff_plus_child', 'cau_eff_mbeff_plus_interaction', 'cau_eff_mbeff_plus_mean', 'cau_eff_mbeff_plus_parent', 'cau_eff_mbeff_plus_std', 'cau_m_eff_interaction', 'cau_m_eff_mean', 'cau_m_eff_std', 'coeff_cause', 'coeff_eff', 'com_cau', 'eff_cau', 'eff_cau_mbcau_plus_interaction', 'eff_cau_mbcau_plus_mean', 'eff_cau_mbcau_plus_std', 'eff_cau_mbeff', 'eff_m_cau_child', 'eff_m_cau_interaction', 'eff_m_cau_mean', 'eff_m_cau_parent', 'eff_m_cau_std', 'errors_correlation_with_inputs', 'kurtosis_ca', 'kurtosis_ef', 'm_cau_interaction', 'm_cau_mean', 'm_cau_std', 'm_eff_child', 'm_eff_interaction', 'm_eff_mean', 'm_eff_parent', 'm_eff_std', 'mbe_mbe_eff_interaction', 'mbe_mbe_eff_mean', 'mbe_mbe_eff_std', 'mca_mca_cau_child', 'mca_mca_cau_interaction', 'mca_mca_cau_mean', 'mca_mca_cau_parent', 'mca_mca_cau_std', 'mca_mef_cau_child', 'mca_mef_cau_interaction', 'mca_mef_cau_mean', 'mca_mef_cau_parent', 'mca_mef_cau_std', 'mca_mef_eff_child', 'mca_mef_eff_interaction', 'mca_mef_eff_mean', 'mca_mef_eff_parent', 'mca_mef_eff_std', 'parcorr_errors', 'skewness_ca', 'skewness_ef', 'te_asymmetry_diff_1_15', 'transfer_entropy_bwd', 'transfer_entropy_diff', 'transfer_entropy_fwd']\n",
      "Running in single-threaded mode or managing parallelism manually.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d03dfdb0ce4976a5cc5c82d9b0c5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Time Series:   0%|          | 0/1050 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2C run complete.\n",
      "NaNs found in the final y_true vector: 0\n",
      "ROC AUC AFTER 0.8251527865834245\n",
      "Successfully saved all final results for NETSIM_5 to: data/causal_dfs/causal_dfs_NETSIM_5.pkl\n",
      "\n",
      "============================================================\n",
      "--- PROCESSING DATASET: NETSIM_10 ---\n",
      "Variables: 10, Max Lags: 3\n",
      "Found cached competitor results. Loading from: data/before_d2c/causal_dfs_before_d2c_NETSIM_10.pkl\n",
      "Running D2C for NETSIM_10...\n",
      "  - Using precomputed descriptors from: data/descriptors/descriptors_netsim_10.pkl\n",
      "ROC AUC BEFORE: 0.8750867533708824\n",
      "Loading and pre-computing predictions from: data/descriptors/descriptors_netsim_10.pkl\n",
      "--- DEBUGGING D2CWRAPPER ---\n",
      "Initial descriptor columns (67): ['HOC_1_2', 'HOC_1_3', 'HOC_2_1', 'HOC_3_1', 'cau_eff', 'cau_eff_mbcau', 'cau_eff_mbeff_plus_child', 'cau_eff_mbeff_plus_interaction', 'cau_eff_mbeff_plus_mean', 'cau_eff_mbeff_plus_parent', 'cau_eff_mbeff_plus_std', 'cau_m_eff_interaction', 'cau_m_eff_mean', 'cau_m_eff_std', 'coeff_cause', 'coeff_eff', 'com_cau', 'edge_dest', 'edge_source', 'eff_cau', 'eff_cau_mbcau_plus_interaction', 'eff_cau_mbcau_plus_mean', 'eff_cau_mbcau_plus_std', 'eff_cau_mbeff', 'eff_m_cau_child', 'eff_m_cau_interaction', 'eff_m_cau_mean', 'eff_m_cau_parent', 'eff_m_cau_std', 'errors_correlation_with_inputs', 'graph_id', 'is_causal', 'kurtosis_ca', 'kurtosis_ef', 'm_cau_interaction', 'm_cau_mean', 'm_cau_std', 'm_eff_child', 'm_eff_interaction', 'm_eff_mean', 'm_eff_parent', 'm_eff_std', 'mbe_mbe_eff_interaction', 'mbe_mbe_eff_mean', 'mbe_mbe_eff_std', 'mca_mca_cau_child', 'mca_mca_cau_interaction', 'mca_mca_cau_mean', 'mca_mca_cau_parent', 'mca_mca_cau_std', 'mca_mef_cau_child', 'mca_mef_cau_interaction', 'mca_mef_cau_mean', 'mca_mef_cau_parent', 'mca_mef_cau_std', 'mca_mef_eff_child', 'mca_mef_eff_interaction', 'mca_mef_eff_mean', 'mca_mef_eff_parent', 'mca_mef_eff_std', 'parcorr_errors', 'skewness_ca', 'skewness_ef', 'te_asymmetry_diff_1_15', 'transfer_entropy_bwd', 'transfer_entropy_diff', 'transfer_entropy_fwd']\n",
      "Model feature names (63): ['HOC_1_2', 'HOC_1_3', 'HOC_2_1', 'HOC_3_1', 'cau_eff', 'cau_eff_mbcau', 'cau_eff_mbeff_plus_child', 'cau_eff_mbeff_plus_interaction', 'cau_eff_mbeff_plus_mean', 'cau_eff_mbeff_plus_parent', 'cau_eff_mbeff_plus_std', 'cau_m_eff_interaction', 'cau_m_eff_mean', 'cau_m_eff_std', 'coeff_cause', 'coeff_eff', 'com_cau', 'eff_cau', 'eff_cau_mbcau_plus_interaction', 'eff_cau_mbcau_plus_mean', 'eff_cau_mbcau_plus_std', 'eff_cau_mbeff', 'eff_m_cau_child', 'eff_m_cau_interaction', 'eff_m_cau_mean', 'eff_m_cau_parent', 'eff_m_cau_std', 'errors_correlation_with_inputs', 'kurtosis_ca', 'kurtosis_ef', 'm_cau_interaction', 'm_cau_mean', 'm_cau_std', 'm_eff_child', 'm_eff_interaction', 'm_eff_mean', 'm_eff_parent', 'm_eff_std', 'mbe_mbe_eff_interaction', 'mbe_mbe_eff_mean', 'mbe_mbe_eff_std', 'mca_mca_cau_child', 'mca_mca_cau_interaction', 'mca_mca_cau_mean', 'mca_mca_cau_parent', 'mca_mca_cau_std', 'mca_mef_cau_child', 'mca_mef_cau_interaction', 'mca_mef_cau_mean', 'mca_mef_cau_parent', 'mca_mef_cau_std', 'mca_mef_eff_child', 'mca_mef_eff_interaction', 'mca_mef_eff_mean', 'mca_mef_eff_parent', 'mca_mef_eff_std', 'parcorr_errors', 'skewness_ca', 'skewness_ef', 'te_asymmetry_diff_1_15', 'transfer_entropy_bwd', 'transfer_entropy_diff', 'transfer_entropy_fwd']\n",
      "Running in single-threaded mode or managing parallelism manually.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbfd426349ac42fbb92d7d158717fffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Time Series:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2C run complete.\n",
      "NaNs found in the final y_true vector: 0\n",
      "ROC AUC AFTER 0.8750867533708824\n",
      "Successfully saved all final results for NETSIM_10 to: data/causal_dfs/causal_dfs_NETSIM_10.pkl\n",
      "\n",
      "============================================================\n",
      "--- PROCESSING DATASET: TEST ---\n",
      "Variables: 5, Max Lags: 3\n",
      "Found cached competitor results. Loading from: data/before_d2c/causal_dfs_before_d2c_TEST.pkl\n",
      "Running D2C for TEST...\n",
      "  - Using precomputed descriptors from: data/descriptors/descriptors_df_test.pkl\n",
      "ROC AUC BEFORE: 0.8626967139309651\n",
      "Loading and pre-computing predictions from: data/descriptors/descriptors_df_test.pkl\n",
      "--- DEBUGGING D2CWRAPPER ---\n",
      "Initial descriptor columns (67): ['HOC_1_2', 'HOC_1_3', 'HOC_2_1', 'HOC_3_1', 'cau_eff', 'cau_eff_mbcau', 'cau_eff_mbeff_plus_child', 'cau_eff_mbeff_plus_interaction', 'cau_eff_mbeff_plus_mean', 'cau_eff_mbeff_plus_parent', 'cau_eff_mbeff_plus_std', 'cau_m_eff_interaction', 'cau_m_eff_mean', 'cau_m_eff_std', 'coeff_cause', 'coeff_eff', 'com_cau', 'edge_dest', 'edge_source', 'eff_cau', 'eff_cau_mbcau_plus_interaction', 'eff_cau_mbcau_plus_mean', 'eff_cau_mbcau_plus_std', 'eff_cau_mbeff', 'eff_m_cau_child', 'eff_m_cau_interaction', 'eff_m_cau_mean', 'eff_m_cau_parent', 'eff_m_cau_std', 'errors_correlation_with_inputs', 'graph_id', 'is_causal', 'kurtosis_ca', 'kurtosis_ef', 'm_cau_interaction', 'm_cau_mean', 'm_cau_std', 'm_eff_child', 'm_eff_interaction', 'm_eff_mean', 'm_eff_parent', 'm_eff_std', 'mbe_mbe_eff_interaction', 'mbe_mbe_eff_mean', 'mbe_mbe_eff_std', 'mca_mca_cau_child', 'mca_mca_cau_interaction', 'mca_mca_cau_mean', 'mca_mca_cau_parent', 'mca_mca_cau_std', 'mca_mef_cau_child', 'mca_mef_cau_interaction', 'mca_mef_cau_mean', 'mca_mef_cau_parent', 'mca_mef_cau_std', 'mca_mef_eff_child', 'mca_mef_eff_interaction', 'mca_mef_eff_mean', 'mca_mef_eff_parent', 'mca_mef_eff_std', 'parcorr_errors', 'skewness_ca', 'skewness_ef', 'te_asymmetry_diff_1_15', 'transfer_entropy_bwd', 'transfer_entropy_diff', 'transfer_entropy_fwd']\n",
      "Model feature names (63): ['HOC_1_2', 'HOC_1_3', 'HOC_2_1', 'HOC_3_1', 'cau_eff', 'cau_eff_mbcau', 'cau_eff_mbeff_plus_child', 'cau_eff_mbeff_plus_interaction', 'cau_eff_mbeff_plus_mean', 'cau_eff_mbeff_plus_parent', 'cau_eff_mbeff_plus_std', 'cau_m_eff_interaction', 'cau_m_eff_mean', 'cau_m_eff_std', 'coeff_cause', 'coeff_eff', 'com_cau', 'eff_cau', 'eff_cau_mbcau_plus_interaction', 'eff_cau_mbcau_plus_mean', 'eff_cau_mbcau_plus_std', 'eff_cau_mbeff', 'eff_m_cau_child', 'eff_m_cau_interaction', 'eff_m_cau_mean', 'eff_m_cau_parent', 'eff_m_cau_std', 'errors_correlation_with_inputs', 'kurtosis_ca', 'kurtosis_ef', 'm_cau_interaction', 'm_cau_mean', 'm_cau_std', 'm_eff_child', 'm_eff_interaction', 'm_eff_mean', 'm_eff_parent', 'm_eff_std', 'mbe_mbe_eff_interaction', 'mbe_mbe_eff_mean', 'mbe_mbe_eff_std', 'mca_mca_cau_child', 'mca_mca_cau_interaction', 'mca_mca_cau_mean', 'mca_mca_cau_parent', 'mca_mca_cau_std', 'mca_mef_cau_child', 'mca_mef_cau_interaction', 'mca_mef_cau_mean', 'mca_mef_cau_parent', 'mca_mef_cau_std', 'mca_mef_eff_child', 'mca_mef_eff_interaction', 'mca_mef_eff_mean', 'mca_mef_eff_parent', 'mca_mef_eff_std', 'parcorr_errors', 'skewness_ca', 'skewness_ef', 'te_asymmetry_diff_1_15', 'transfer_entropy_bwd', 'transfer_entropy_diff', 'transfer_entropy_fwd']\n",
      "Running in single-threaded mode or managing parallelism manually.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9108bde89e4c8f895c40960d2bd913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Time Series:   0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2C run complete.\n",
      "NaNs found in the final y_true vector: 0\n",
      "ROC AUC AFTER 0.8626967139309651\n",
      "Successfully saved all final results for TEST to: data/causal_dfs/causal_dfs_TEST.pkl\n",
      "\n",
      "============================================================\n",
      "--- All benchmark runs completed successfully! ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# --- 3. MAIN BENCHMARKING LOOP ---\n",
    "# ==============================================================================\n",
    "for config in DATASETS_TO_PROCESS:\n",
    "    dataset_name = config[\"name\"]\n",
    "    n_vars = config[\"n_vars\"]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"--- PROCESSING DATASET: {dataset_name} ---\")\n",
    "    print(f\"Variables: {n_vars}, Max Lags: {MAXLAGS}\")\n",
    "\n",
    "    if dataset_name == \"TEST\": \n",
    "        #special loading helper: it's split in multiple files\n",
    "        #one per kind of error, and we need to make sure the loading \n",
    "        #order and merging are exactly the same as when we have computed\n",
    "        #descriptors (notebook 01_descriptors_computation.ipynb)\n",
    "        original_observations_testing, true_causal_dfs = _load_testing_data()\n",
    "    else:\n",
    "        dataloader = DataLoader(n_variables=n_vars, maxlags=MAXLAGS)\n",
    "        dataloader.from_pickle(config[\"input_file\"])\n",
    "        original_observations_testing = dataloader.get_original_observations()\n",
    "        true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "    # --- Run Competitors (or load from cache) ---\n",
    "    competitors_cache_path = PRE_RESULTS_DIR / f'causal_dfs_before_d2c_{dataset_name}.pkl'\n",
    "    \n",
    "    if competitors_cache_path.exists():\n",
    "        print(f\"Found cached competitor results. Loading from: {competitors_cache_path}\")\n",
    "        with open(competitors_cache_path, 'rb') as f:\n",
    "            all_competitors = pickle.load(f)\n",
    "        causal_dfs_var = all_competitors['causal_dfs_var']\n",
    "        causal_dfs_varlingam = all_competitors['causal_dfs_varlingam']\n",
    "        causal_dfs_pcmci = all_competitors['causal_dfs_pcmci']\n",
    "        causal_dfs_pcmci_gpdc = all_competitors['causal_dfs_pcmci_gpdc']\n",
    "        causal_dfs_granger = all_competitors['causal_dfs_granger']\n",
    "        causal_dfs_mvgc = all_competitors['causal_dfs_mvgc']\n",
    "        causal_dfs_dynotears = all_competitors['causal_dfs_dynotears']\n",
    "    else:\n",
    "        print(f\"No cached results found. Running all competitor benchmarks for {dataset_name}...\")\n",
    "        \n",
    "        # Instantiate and run all competitor models\n",
    "        var = VAR(ts_list=original_observations_testing, maxlags=MAXLAGS, n_jobs=N_JOBS); var.run()\n",
    "        varlingam = VARLiNGAM(ts_list=original_observations_testing, maxlags=MAXLAGS, n_jobs=N_JOBS); varlingam.run()\n",
    "        pcmci = PCMCI(ts_list=original_observations_testing, maxlags=MAXLAGS, n_jobs=N_JOBS); pcmci.run()\n",
    "        pcmci_gpdc = PCMCI(ts_list=original_observations_testing, maxlags=MAXLAGS, n_jobs=N_JOBS, ci='GPDC'); pcmci_gpdc.run()\n",
    "        granger = Granger(ts_list=original_observations_testing, maxlags=MAXLAGS, n_jobs=N_JOBS); granger.run()\n",
    "        dynotears = DYNOTEARS(ts_list=original_observations_testing, maxlags=MAXLAGS, n_jobs=N_JOBS); dynotears.run()\n",
    "        mvgc = MultivariateGranger(ts_list=original_observations_testing, maxlags=MAXLAGS, n_jobs=N_JOBS); mvgc.run()\n",
    "        \n",
    "        # Get results\n",
    "        causal_dfs_var = var.get_causal_dfs()\n",
    "        causal_dfs_varlingam = varlingam.get_causal_dfs()\n",
    "        causal_dfs_pcmci = pcmci.get_causal_dfs()\n",
    "        causal_dfs_pcmci_gpdc = pcmci_gpdc.get_causal_dfs()\n",
    "        causal_dfs_granger = granger.get_causal_dfs()\n",
    "        causal_dfs_mvgc = mvgc.get_causal_dfs()\n",
    "        causal_dfs_dynotears = dynotears.get_causal_dfs()\n",
    "        \n",
    "        # Save to cache for next time\n",
    "        all_to_cache = {\n",
    "            'causal_dfs_var': causal_dfs_var, 'causal_dfs_varlingam': causal_dfs_varlingam,\n",
    "            'causal_dfs_pcmci': causal_dfs_pcmci, 'causal_dfs_pcmci_gpdc': causal_dfs_pcmci_gpdc,\n",
    "            'causal_dfs_granger': causal_dfs_granger, 'causal_dfs_mvgc': causal_dfs_mvgc,\n",
    "            'causal_dfs_dynotears': causal_dfs_dynotears,\n",
    "        }\n",
    "        with open(competitors_cache_path, 'wb') as f:\n",
    "            pickle.dump(all_to_cache, f)\n",
    "        print(f\"Competitor results saved to cache: {competitors_cache_path}\")\n",
    "\n",
    "    # --- Run D2C ---\n",
    "    print(f\"Running D2C for {dataset_name}...\")\n",
    "    d2c_descriptors_path = DESCRIPTORS_DIR / config[\"d2c_descriptors_file\"]\n",
    "    d2c_args = {\n",
    "        \"ts_list\": original_observations_testing,\n",
    "        \"model\": clf,\n",
    "        \"threshold\": THRESHOLD,  \n",
    "        \"n_variables\": n_vars,\n",
    "        \"maxlags\": MAXLAGS,\n",
    "        \"mb_estimator\": 'ts',\n",
    "        \"manages_own_parallelism\": True, # False to paralellelize per observation, True to parallelize per couple\n",
    "    }\n",
    "\n",
    "    # Conditionally add the precomputed descriptors path\n",
    "    if d2c_descriptors_path.exists():\n",
    "        print(f\"  - Using precomputed descriptors from: {d2c_descriptors_path}\")\n",
    "        d2c_args[\"precomputed_descriptors_path\"] = str(d2c_descriptors_path)\n",
    "        d2c_args[\"n_jobs\"] = 1\n",
    "    else:\n",
    "        print(f\"  - WARNING: Precomputed descriptors not found at {d2c_descriptors_path}.\")\n",
    "        print(\"  - D2C will compute descriptors on the fly. This may be slow.\")\n",
    "        d2c_args[\"n_jobs\"] = N_JOBS\n",
    "\n",
    "\n",
    "\n",
    "    test = pd.read_pickle(d2c_args[\"precomputed_descriptors_path\"])\n",
    "    test.fillna(0, inplace=True)\n",
    "    X_test = test.drop(columns=[\"graph_id\", \"edge_source\", \"edge_dest\", \"is_causal\"], errors='ignore')\n",
    "    y_test = test['is_causal']\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    print(f'ROC AUC BEFORE: {roc_auc_score(y_test, y_pred_proba)}')\n",
    "\n",
    "    d2cwrapper = D2CWrapper(**d2c_args)\n",
    "    d2cwrapper.run()\n",
    "    causal_dfs_d2c = d2cwrapper.get_causal_dfs()\n",
    "    print(\"D2C run complete.\")\n",
    "    \n",
    "    # --- Save Final Results ---\n",
    "    # Helper to sort dictionaries by key for consistent ordering\n",
    "    def sort_dict_by_key(d):\n",
    "        return {k: d[k] for k in sorted(d)} if isinstance(d, dict) else d\n",
    "\n",
    "    final_output_path = RESULTS_DIR / f'causal_dfs_{dataset_name}.pkl'\n",
    "    \n",
    "    # Add key to true_causal_dfs for uniformity\n",
    "    true_causal_dfs_cleaned = {k: v for k, v in enumerate(true_causal_dfs) if v is not None}\n",
    "    \n",
    "    # Collect all results into a tuple\n",
    "    final_results = (\n",
    "        sort_dict_by_key(causal_dfs_var),\n",
    "        sort_dict_by_key(causal_dfs_varlingam),\n",
    "        sort_dict_by_key(causal_dfs_pcmci),\n",
    "        sort_dict_by_key(causal_dfs_mvgc),\n",
    "        sort_dict_by_key(causal_dfs_pcmci_gpdc),\n",
    "        sort_dict_by_key(causal_dfs_granger),\n",
    "        sort_dict_by_key(causal_dfs_dynotears),\n",
    "        sort_dict_by_key(causal_dfs_d2c),\n",
    "        true_causal_dfs_cleaned\n",
    "    )\n",
    "    \n",
    "    prediction_truth_df = prepare_prediction_df_d2c(causal_dfs_d2c, true_causal_dfs_cleaned)\n",
    "    \n",
    "    # ADD THIS DEBUG LINE:\n",
    "    print(f\"NaNs found in the final y_true vector: {prediction_truth_df.y_true.isna().sum()}\")\n",
    "\n",
    "    print(\"ROC AUC AFTER\", roc_auc_score(prediction_truth_df.y_true, prediction_truth_df.y_pred))\n",
    "\n",
    "\n",
    "    with open(final_output_path, 'wb') as f:\n",
    "        pickle.dump(final_results, f)\n",
    "    \n",
    "    print(f\"Successfully saved all final results for {dataset_name} to: {final_output_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"--- All benchmark runs completed successfully! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "td2c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
